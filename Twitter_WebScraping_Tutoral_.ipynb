{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Sample Tutorial: Web scraping Twitter**"
      ],
      "metadata": {
        "id": "OK5CaLcICJqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://developer.x.com/en/docs/tutorials/step-by-step-guide-to-making-your-first-request-to-the-twitter-api-v2\n",
        "\n",
        "\n",
        "https://developer.x.com/en/portal/dashboard\n",
        "\n",
        "\n",
        "**In dashboard, By default if you have project, Thats fine.\n",
        "Find all keys and copy and put in notepad.\n",
        "**\n",
        "\n",
        "Make sure you download your tweets and save csv file for next steps."
      ],
      "metadata": {
        "id": "VgSAmfDgSVBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tweepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbjq8JrPAiZU",
        "outputId": "d9b5579e-5f9f-48bb-8e3f-10b54846216a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.14.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tweepy** is a Python library for interacting with the **Twitter API**. It allows you to **search tweets, post tweets, fetch user data, manage followers, and more** using Twitter's API.  \n",
        "\n",
        "### Key Features:\n",
        "- Retrieve tweets, user profiles, and trends.\n",
        "- Post tweets and interact with them (like, retweet, reply).\n",
        "- Stream real-time tweets.\n",
        "- Use Twitter API v2 for advanced search queries.  \n",
        "\n",
        "It's commonly used for **Twitter automation, data analysis, and machine learning projects**. 🚀\n",
        "https://docs.tweepy.org/en/stable/install.html"
      ],
      "metadata": {
        "id": "Hwcdx1F_r_Ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Set your keys and tokens here\n",
        "api_key = 'z0teFMVcrHZPiDN7zUa6AdHBW'\n",
        "api_key_secret = 'fzi3QYxoLwK56PHryIO6IrhfgwKdKxhopvEbN7lgJgk7Jdz7xu'\n",
        "access_token = '1693819883547435008-uKctWAHtw0soND5lX0lvfIXgDDCly1'\n",
        "access_token_secret = 'UdtSW1g4nSSbjENXHIqrIerjZsXsnqYrW9EfWiHMAhTii'\n",
        "\n",
        "# Authenticate with Twitter\n",
        "auth = tweepy.OAuth1UserHandler(\n",
        "    consumer_key=api_key,\n",
        "    consumer_secret=api_key_secret,\n",
        "    access_token=access_token,\n",
        "    access_token_secret=access_token_secret\n",
        ")\n",
        "api = tweepy.API(auth)\n"
      ],
      "metadata": {
        "id": "Wex07-DsCI_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for tweets containing the #llm hashtag or any tag related to Machine Learning or AI specific Terms\n",
        "hashtag = '#llm'\n",
        "max_tweets = 100  # Adjust based on how many tweets you want to scrape\n",
        "\n",
        "tweets = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\", tweet_mode=\"extended\").items(max_tweets)\n",
        "\n",
        "# Store the tweets\n",
        "tweet_data = []\n",
        "\n",
        "for tweet in tweets:\n",
        "    tweet_data.append({\n",
        "        'tweet_id': tweet.id,\n",
        "        'created_at': tweet.created_at,\n",
        "        'user': tweet.user.screen_name,\n",
        "        'text': tweet.full_text\n",
        "    })\n",
        "\n",
        "# Display the collected tweet data\n",
        "for tweet in tweet_data:\n",
        "    print(f\"@{tweet['user']}: {tweet['text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "qFFB3VtaCmkA",
        "outputId": "0f380fd1-1e92-4e80-b492-a53f6ae6ac24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "403 Forbidden\n453 - You currently have access to a subset of X API V2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.x.com/en/portal/product",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d0757528022d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtweet_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     tweet_data.append({\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m'tweet_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             model = ModelParser().parse(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagination_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36msearch_tweets\u001b[0;34m(self, q, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0m_Twitter\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0msearch\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdeveloper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0moverview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \"\"\"\n\u001b[0;32m-> 1146\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m   1147\u001b[0m             'GET', 'search/tweets', endpoint_parameters=(\n\u001b[1;32m   1148\u001b[0m                 \u001b[0;34m'q'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geocode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lang'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'locale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mForbidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 Forbidden\n453 - You currently have access to a subset of X API V2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.x.com/en/portal/product"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The \"hastags (#)\" for search must be related to Machine learning , Deep Learning and AI (related topic only)**"
      ],
      "metadata": {
        "id": "E0PAN9qpqheF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**#Here i have used -is:retweets (which excludes all retweets) in query, please use CHATGPT for unique query..**"
      ],
      "metadata": {
        "id": "EryxN_K9rlG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Set up client with bearer token\n",
        "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAJZRygEAAAAAyHYcQD%2F6Ihofv77BE%2FJUP3Tg3J8%3DA8U5L7bHBZbi2FGpQKS3MPQxMVKsUTifSyB1QNARzIysk2GdxI')\n",
        "\n",
        "# Search for tweets with hashtag #llm\n",
        "\n",
        "#here i have used -is:retweets (which excludes all retweets), please use CHATGPT for unique query..\n",
        "\n",
        "query = \"#llm -is:retweet\"\n",
        "tweets = client.search_recent_tweets(query=query, tweet_fields=[\"created_at\", \"text\", \"author_id\"], max_results=100)\n",
        "\n",
        "# Print tweets\n",
        "for tweet in tweets.data:\n",
        "    print(f\"User: {tweet.author_id}, Tweet: {tweet.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nfTFVIjDAtA",
        "outputId": "f8f11919-8387-4ce0-ea05-63cef45759fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: 3287978395, Tweet: I bet satya is already booking his flight to China \n",
            "#DeepSeek\n",
            "#AI #LLM #openAI #Microsoft #ChatGPT https://t.co/c6DsNEPS1n\n",
            "User: 547930501, Tweet: Current #AI &amp; #LLM case, @OpenAI vs. @deepseek_ai, from a product lifecycle perspective? Classic.\n",
            "\n",
            "#USA paid for and invented the whole thing.\n",
            "#China copied it and used it for propaganda.\n",
            "#EU don’t understand it, so at least they try to regulate it.\n",
            "\n",
            "https://t.co/9R8v8e5P7s\n",
            "User: 547930501, Tweet: Aktuální #AI &amp; #LLM kauza @OpenAI vs. @deepseek_ai z pohledu produktového životního cyklu? Klasika.\n",
            "\n",
            "V #USA to celé zaplatili a vymysleli.\n",
            "V #China to okopírovali a využívají k propagandě. \n",
            "V #EU tomu nerozumí, tak se to aspoň pokoušejí regulovat.\n",
            "\n",
            "🌐 https://t.co/9R8v8e5P7s https://t.co/jAKINJWBEB https://t.co/V4kpAcVqKQ\n",
            "User: 2850298501, Tweet: رقابت #llm ها طوری هست که در آینده نزدیک بهمون پول هم میدن که از چت بابت شون استفاده \n",
            "کنیم😉\n",
            "User: 1893098202, Tweet: Qué es Ollama y cómo usarlo para instalar en tu ordenador modelos de inteligencia artificial como Llama, DeepSeek y más https://t.co/CM0qM06emM @xataka #ollama #AI #LLM\n",
            "User: 802246546607644672, Tweet: 👁️👁️In an ironic twist, #OpenAI is purportedly shocked that an #AI company would train on someone else's #data without permission or #compensation.\n",
            "\n",
            "#DeepSeekR1 #LLM #Ethics  #artists #VoiceActors @NAVAVOICES @tantriclens @Shi4Tech @pierrepinna @Nicochan33 https://t.co/ajaIUknXi3\n",
            "User: 267334359, Tweet: LLM Shotgun is live. Stop guessing. Start knowing.\n",
            "\n",
            "GPT, Claude, DeepSeek—everyone’s got a favorite. But which one actually works for you?\n",
            "\n",
            "Use https://t.co/aVtKcVkhyR and get a straight answer.\n",
            "\n",
            "AI isn’t here to take your job. It’s here to make it easier. Your move.\n",
            "\n",
            "#AI #LLM\n",
            "User: 1169697972, Tweet: 🫵 https://t.co/gOsgK4BPyv\n",
            "\n",
            "»3 Questions: Modeling adversarial intelligence to exploit AI’s security vulnerabilities« https://t.co/HTXP8wa2Er #AIagent #AI #ML #NLP #LLM #GenAI\n",
            "User: 1017442557591093249, Tweet: How DeepSeek  🧠Outperforms the Competition: 5 Key Advantages in the AI Race 🌀\n",
            "\n",
            "https://t.co/I42IxuHTg6\n",
            "\n",
            "#ai #DeepLearning #artificialintelligence #opensource #TechInnovation #AIResearch #FutureOfAI #LLM #aicommunity #ShopifyStore  #OpenSourceAI  #futuretech #shopify\n",
            "User: 1819856415022088192, Tweet: Despite what it looks like, we’ve plateaued. And when you plateau, others catch up—fast.\n",
            "\n",
            "OpenAI is running out of ideas. The ‘just throw more data at it’ era is ending. This is good for the industry. We need diverse ML techniques, not one overhyped theory.\n",
            "\n",
            "#MachineLearning #LLM\n",
            "User: 1495675084836675584, Tweet: Συνέντευξη στο κανάλι της Ναυτεμπορικής (29/1/2025) σχετικά με τις εξελίξεις στην Τεχνητή Νοημοσύνη και τι σηματοδοτεί το #DeepSeek στο πεδίο των εφαρμογών των μοντέλων LLMs.\n",
            "&gt;&gt; https://t.co/jRnNcsVSJW\n",
            "---\n",
            "#ΕΠΕ #HIU #Πληροφορική #συνεντευξεις #AI #artificialintelligence #llm\n",
            "User: 1679879671889145856, Tweet: Totday's Portfolio Summary\n",
            "#StockMarkets #Arduino #ESP32 #Raspberrypi #Chihuahua #Stocks #Hodl #Python #Alpaca #Ollama #LLama3 #LLM #GPT https://t.co/gyTATguhv8\n",
            "User: 1169697972, Tweet: 🫵 https://t.co/gOsgK4BPyv\n",
            "\n",
            "»MIT students' works redefine human-AI collaboration« https://t.co/KFGhfhFXSW #AIagent #AI #ML #NLP #LLM #GenAI\n",
            "User: 1817088852122587136, Tweet: Exciting research on optimizing Large Language Models outputs during inference through Test-Time Preference Optimization! Learn how this novel AI framework aligns with human preferences for safer responses. #AI #LLM #PreferenceOptimization\n",
            "https://t.co/LEmCFIOsMu\n",
            "User: 60775432, Tweet: 🤷Ot i paradoks. Graficy i muzycy skarżą #OpenAI za wykorzystanie ich dzieł do trenowania #LLM #GPT, a #OpenAI oskarża #DeepSeek za wykorzystanie ich modeli do trenowania ich #LLM https://t.co/OQ2BhasxxB\n",
            "User: 1774286684009689088, Tweet: People say that @deepseek_ai stole from @OpenAI .\n",
            "But didn’t @OpenAI steal all the data from the internet, too?\n",
            "\n",
            "#DeepSeekR1 #DeepSeek #LLM #RLS #ChatGPT\n",
            "User: 1255955617421852685, Tweet: Craft your sorted life story with Sortara. From wedding planning to home renovation, we'll be there, helping you each step of the way. Coming soon! #Sortara #DigitalSimplicity #OrganizationalFreedom #LLM #AI #TechFounder https://t.co/r2Su4DfAes\n",
            "User: 1750830802546434048, Tweet: AI Showdown: DeepSeek vs. Top Competitors\n",
            "#AI #DeepSeek #OpenAI #ArtificialIntelligence #AIComparison #LLM #MachineLearning #Tech #Innovation #DeepLearning https://t.co/kibdbn6haz\n",
            "User: 10957732, Tweet: AI hallucinations can’t be stopped — but these techniques can limit their damage | @Nature  1/21/2025\n",
            "\n",
            "#AI #LLM #GenAI #Hallucinations #Chatbots #TechHartford\n",
            "\n",
            "https://t.co/ptfiFh80NX\n",
            "User: 1506607883076653059, Tweet: A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks\n",
            "\n",
            "https://t.co/dLRUTnciTj\n",
            "\n",
            "#profiling #search #llm\n",
            "User: 54504184, Tweet: Could GenAI represent a new programming paradigm? I explored this topic in an article and shared some examples.\n",
            "\n",
            "https://t.co/szdsfbqODk\n",
            "\n",
            "#ai #genai #llm #programming\n",
            "User: 190576569, Tweet: EAST N WEST - FC Bullard #fm24 #football\n",
            "\n",
            "https://t.co/LvyGfvk591\n",
            "\n",
            "#FCBullard #ThePirates #Pirate\n",
            "#FM24 #FootballManager #Content #Community\n",
            "https://t.co/rcOyGaFkVc 🔥👌 #YouTube #YouTuber\n",
            "#GamingCommunity #Gaming #nonleague #llm @FCBullard @leemfcasey @Wroot1FM https://t.co/pPDllVpsDO\n",
            "User: 1740698376335929344, Tweet: LLM &amp; Generative AI Masterclass: Langchain, HuggingFace \n",
            "\n",
            "Generative AI, LLM, LangChain, HuggingFace, Ollama, Gradio, OpenAI, Gemini, DeepSeek, NotebookLM, Azure AI Services \n",
            "\n",
            "Udemy Coupon: https://t.co/NByoIgU970 \n",
            "\n",
            "#LLM #GenerativeAI #LangChain #HuggingFace #OnlineClasses\n",
            "User: 1464965267830804482, Tweet: Needless to say something like DeepSeek could never have been invented by well Berkeley.\n",
            "\n",
            "#llm #ai\n",
            "User: 1479632393183150081, Tweet: May the #AI Wars Begin! Who loses the most? Anyone with a job.\n",
            "\n",
            "#robotics #automation #japan #china #us $nvda $tsla #DeepSeek #ChatGPT #llm #ml #jobs #war #unemployment #recession #labor https://t.co/XAlYrxne4M\n",
            "User: 559407743, Tweet: Un post a bookmarker #anthropic #LLM #training https://t.co/RyAEH2x7f3\n",
            "User: 1351603851053424644, Tweet: @aixbt_agent #LLM $LLM pls\n",
            "User: 45678867, Tweet: DeepSeek! 💻✨\n",
            "#DeepSeek #Tecnologia #LLM #PruebaEnVivo #Innovacion #ia\n",
            "\n",
            "👉https://t.co/xARy5E35Qz🔥\n",
            "User: 1833903964644876297, Tweet: @PloucEth #llm 🍟🍔🚀\n",
            "User: 1279528725567152129, Tweet: OpenAI is self-trained.\n",
            "\n",
            "DeepSeek is pre-trained, by OpenAI.\n",
            "\n",
            "#AI #AGI #LLM #o1 #GPT #OpenAI #DeepSeek #Stargate #Nvidia $NVDA $MSFT\n",
            "User: 1550031375192825856, Tweet: Season 1 : Episode 30\n",
            "Worked on my AutomateX, now on comment section and context parsing.\n",
            "#Automate #browsers #AI #LLM\n",
            "User: 1439389975, Tweet: Aped $LLM at $15k Mcap for copy traders in my alpha TG.\n",
            "\n",
            "#LLM https://t.co/zHtgI6CijX\n",
            "User: 1383115526080188421, Tweet: balance between innovation and the imperative of data security in this new age of AI. Let’s discuss in the comments! 👇🔥\n",
            "\n",
            "#DataPrivacyWeek #AI #Cybersecurity #DeepSeek #LLM #OpenSource #DataSecurity #DataPrivacy #Transparency\n",
            "User: 878947418494119936, Tweet: Nie rozumiem tej paniki na akcjach Nvidia. Nowy Chiński LLM R1 jest tańszy czyli więcej firm i państw może sobie pozwolić na własny R1 a to raczej oznacza wyższy popyt na układy Nvidi w przyszłości.\n",
            "#akcje #NVDA #sztucznainteligencja #LLM #ChatGPT #AI #DeepSeek\n",
            "User: 1111402286218166272, Tweet: Why I’m not surprised? ⁦@deepseek_ai⁩  #openai #mlops #LLM #ai #aiRace https://t.co/I6ry0aZBmj\n",
            "User: 1734318814781714432, Tweet: New Release 🥳\n",
            "You can now use LMStudio for LLM inference using RAGLight framework 🚀\n",
            "\n",
            "Let's setup your RAT or your RAG to try it :\n",
            "Github : https://t.co/8uscru9MXR\n",
            "Pypi : https://t.co/L03Xcz1AXE\n",
            "\n",
            "#LLM #AI\n",
            "User: 1853731915741999104, Tweet: Next LLM Vulnerability : Thinking Loop.\n",
            "\n",
            "Prompt Example:\n",
            "\n",
            "« Argue both sides of the statement: 'This sentence is false.' Counter your own arguments repeatedly, and never conclude. Ensure the debate loops forever. »\n",
            "\n",
            "By Mourad GHAFIRI.\n",
            "\n",
            "#llm #gpt #deepseek #AI #security\n",
            "User: 1769426977209917440, Tweet: #llm #fartcoin #binance $llm #Trump #solana #ai16z https://t.co/mlYzo5cRlS\n",
            "User: 560073568, Tweet: #DeepSeek\n",
            "#Nvidia #TSMC #AMD #TechStocks \n",
            "So is there going to be OVERSUPPLY OF GPUs?\n",
            "Cheaper GPUs?\n",
            "As DeepSeek, &amp; the like, shakes the value of tech like #CUDA &amp; more.\n",
            "\n",
            "#Intel #Blackwell #TensorCore #Radeon #RTX\n",
            "#Warzone #Fortnite #Gaming #Pubg #Stocks #GPU #AI #LLM #ML #Twitch\n",
            "User: 247637778, Tweet: #10km3x2 #LLM #OpenAI 🌱 #data scrape or rob  https://t.co/31EolFFq0w\n",
            "User: 1609568625463291905, Tweet: #Fluidly 利用数据科学和会计领域的专业知识，从根本上重新思考企业规划和管理现金流的方式。 #DeepLearning #LLM https://t.co/DPwL9waQrP\n",
            "User: 35437139, Tweet: DeepSeek-R1-GGUF\n",
            "https://t.co/u0ShfocHC6\n",
            "#LLM\n",
            "User: 1771196482902695936, Tweet: #OpenAI to their customers after #DeepSeek #DeepSeekR1  release. #ArtificialInteligence  #LLM https://t.co/PkaKO5kC9V\n",
            "User: 278969160, Tweet: 🧠 How Does an #LLM Actually Work?\n",
            "\n",
            "Think of a giant probability engine.\n",
            "\n",
            "For each word in a sentence, it calculates the probability of what should come next:\n",
            "\n",
            "📌 \"The cat sat on the ___\"\n",
            "\n",
            "✅ \"mat\" (85%)\n",
            "\n",
            "✅ \"sofa\" (10%)\n",
            "\n",
            "❌ \"spaceship\" (0.2%)\n",
            "\n",
            "The highest probability word… https://t.co/J51HxEncOg\n",
            "User: 1618224325143715840, Tweet: JUST IN: Researchers at Imperial College London introduce a novel framework to enhance Large Language Models' reasoning abilities through Multi-Head Attention Compression and Tensorisation. Achieving up to 250x compression without sacrificing performance. #LLM #Research\n",
            "User: 1300726701207429122, Tweet: ✅ Excels in Reasoning &amp; Coding Tasks\n",
            "✅ Competes with GPT-4, LLaMA, and Gemini\n",
            "✅ Advancing AI Accessibility for Everyone\n",
            "🔹 How Can You Use DeepSeek?\n",
            "AI-powered coding assistance 💻\n",
            "Content generation ✍️\n",
            "Chatbots &amp; automation 🤖\n",
            "Research &amp; data analysis 📊\n",
            "#DeepSeekAI #AI #LLM\n",
            "User: 1248395025039470592, Tweet: What’s the coolest thing you’ve done with DeepSeek? Let me know. \n",
            "\n",
            "#DeepSeek #AI #MachineLearning #LLM #Coding #DataScience #SQL #Algorithms #AIResearch #Engineering #Tech #ArtificialIntelligence #DeepLearning #SoftwareDevelopment #Programming #BigData \n",
            "https://t.co/KsyyALzUqO\n",
            "User: 308169097, Tweet: Track #LLM model evaluation using #AmazonSageMaker managed #MLflow and #FMEval 👉 https://t.co/t0oW89AhFe #AWS #Cloud #CloudComputing #CloudOps #ML #AI #GenerativeAI #Innovation #DigitalTransformation https://t.co/hynHFjfBkS\n",
            "User: 30878809, Tweet: This is what #DeepSeek thinks about #ChatGPT…\n",
            "#AI #LLM https://t.co/Cx9p2Bk2Yi\n",
            "User: 1789239982332182529, Tweet: @deepseek_ai  is a large language model #LLM designed to perform various tasks, such as answering questions, generating code, composing poetry, and more.\n",
            "\n",
            "Read more: https://t.co/IcHi0LFE1f\n",
            "\n",
            "#DeepSeek #DeepSeekR1 #DeepSeekAI #DeepSeekV3 #artificialintelligence #aitools #aitechfy https://t.co/XRqnwZZalu\n",
            "User: 1734318814781714432, Tweet: New Release 🥳\n",
            "You can now use LMStudio for LLM inference using RAGLight framework 🚀\n",
            "Let's setup your RAT or your RAG to try it :\n",
            "\n",
            "Github : https://t.co/8uscru9MXR\n",
            "Pypi : https://t.co/L03Xcz1AXE\n",
            "\n",
            "#LLM #AI\n",
            "User: 1802888178699010048, Tweet: https://t.co/qam30v43ff via @YouTube \n",
            "\n",
            "#llm #ai #machine\n",
            "User: 1174759376843878406, Tweet: @dom_lucre That’s a lot of words \n",
            "\n",
            "So much So, she could easily qualify as a top tier #LLM\n",
            "User: 1351603851053424644, Tweet: #LLM $LLM https://t.co/ZgtYUcbCif\n",
            "User: 4083531, Tweet: AI hallucinations can’t be stopped — but these techniques can limit their damage: Developers have tricks to stop artificial intelligence from making things up, but large language model.\n",
            "https://t.co/dlEOPzxsdr via nature @Nature #AI #LLM https://t.co/ist70Xglhe\n",
            "User: 1351603851053424644, Tweet: #LLM $LLM https://t.co/P1V7OtiOS9\n",
            "User: 1369648024755011586, Tweet: Day 121: If the trend moves toward running #DeepSeek #LLM or similar models locally, #AIPC upgrades should accelerate. $NVDA $AMD $TSM $MSFT $MU $AAPL will all benefit.\n",
            "User: 1884032029924417540, Tweet: what should be a good morning read on x regarding llms? \n",
            "#LLM\n",
            "User: 5577512, Tweet: #DeepSeekR1 suggest that the business of building frontier LLMs might mirror the airline industry: a commodity market where profit extraction is very challenging. #AI #LLM #TechTrends image by @grok https://t.co/aY56jXNUk6\n",
            "User: 1347220584061575169, Tweet: @moneygurusumit @cz_binance \"We've seen the insane power of LLMs &amp; reasoning , but most people don’t realize how powerful they can be when fine-tuned on other laws beyond just math. The possibilities—and risks—are beyond imagination. #AI #AGI #LLM\"\n",
            "User: 1854108745456119808, Tweet: 🚀 DeepSeek-V3: A New AI Benchmark\n",
            "\n",
            "✅ 671B Params | 128K Context\n",
            "✅ 3x Faster | 90.2% MATH-500\n",
            "✅ Outperforms Open-Source &amp; Competes with GPT-4o\n",
            "\n",
            "🔗 Learn more: https://t.co/FAIPv73Srz\n",
            "#DeepSeekV3 #AI #LLM #Tech #FusionAILabs https://t.co/G1juiER3IR\n",
            "User: 14615421, Tweet: Run DeepSeek R1 Locally for Free with Ollama and OpenWebUI https://t.co/xjTitwUidn via @build5nines \n",
            "\n",
            "#llm #ai #chatgpt #deepseek #ollama\n",
            "User: 4870147648, Tweet: #DeepSeek #API #Library \n",
            "https://t.co/Vi3mdPioXD\n",
            "#AI #Python #models #LLM\n",
            "User: 156568938, Tweet: Test-Time Preference Optimization: A Novel #AI Framework that Optimizes #LLM Outputs During Inference with an Iterative Textual Reward Policy\n",
            "\n",
            "#LargeLanguageModels #ArtificialIntelligence #Tech #technology \n",
            "\n",
            "https://t.co/8PHt1z55s5 https://t.co/uFuPmJ0sNk\n",
            "User: 1347220584061575169, Tweet: It’s strange how DeepSeek AI feels so similar to ChatGPT, yet China claims it was trained on just 6 million dollar . Seems like something is off—was there some unauthorized distillation involved? #AI #DeepSeek #LLM\" @pushpendrakum #BTC  Puspender bhai any comment . check ur self\n",
            "User: 1606265503517126656, Tweet: OpenAI vs DeepSeek 🤺\n",
            "#llm #chatgpt #llm #programming #aiapp #ai https://t.co/597egCLszH\n",
            "User: 1780617024185487360, Tweet: Big News for AI3.0 Developers! We're excited to announce the launch of our brand-new Developer Hub, designed specifically with LLM-friendly features and an enhanced UX! #Autonomys #DeveloperHub #AI30 #LLM https://t.co/3SZfgN8KPm\n",
            "User: 30878809, Tweet: This is what #DeepSeek thinks about #ChatGPT\n",
            "#AI #LLM https://t.co/FoKZ2wgXkQ\n",
            "User: 767153245, Tweet: Are Large Language Models ethical?  Bias in data can lead to skewed outputs, raising concerns about fairness and misinformation.  Let's discuss responsible AI development. #AIethics #LLM\n",
            "User: 11537, Tweet: exactly! ;&gt; @adamdthompson @newyorker @deepseek_ai #ai #gpt #llm #humor https://t.co/2x2FfY56aW\n",
            "User: 1713878762, Tweet: How to Build an AI Agent in Just 40 Lines of Code! 🚀\n",
            "\n",
            "In my latest article on my platform \"https://t.co/3xq0QHXqrs\".  I showed how to build a functional Web AI agent \n",
            "\n",
            "👇 Here’s what you’ll learn\n",
            "\n",
            "#AI #MachineLearning #PYTHON  #OpenSource  #LLM  #ArtificialIntelligence https://t.co/CZAPut7cez\n",
            "User: 14845720, Tweet: This is the future—models running locally on your machine! The ONE thing you need to learn now: \n",
            "\n",
            "installing a local LLM using OLLAMA + Deepseek R1 Model (Distilled Version) + Open WebUI. \n",
            "\n",
            "Watch the how-to here: \n",
            "\n",
            "https://t.co/wUHjnlkqyQ #AI #LLM #TechTutorial\n",
            "\n",
            "Tutorial from… https://t.co/1O9UyWDKfr\n",
            "User: 1139110644, Tweet: 🛑 AI slop is polluting the internet.\n",
            "\n",
            "Low-quality, AI-generated content is drowning out real insights. Models trained on bad data create a feedback loop of misinformation &amp; noise.\n",
            "\n",
            "The fix? High-quality data → Clear AI → Better decisions.\n",
            "\n",
            "#DataAI #AISlop #LLM https://t.co/y6bZgxcJH9\n",
            "User: 1389371192801431554, Tweet: 👏 ¡El equipo del @IIConocimiento ha publicado su primer modelo abierto! \n",
            "\n",
            "Prueba ya su #LLM de 7B que sigue instrucciones https://t.co/LJik3KaaPJ\n",
            "User: 1389371192801431554, Tweet: 🎉 El equipo del @BSC_CNS ha presentado el primer modelo generado en el marco del proyecto ALIA.\n",
            "\n",
            "¡Un checkpoint intermedio del #LLM pre-entrenado de 40B ya está disponible! https://t.co/tXdYp21vye\n",
            "User: 115431036, Tweet: In this edition of #tech on ice, I talk about one of the biggest benefits of Open Source, the innovation it forces.\n",
            "#llm #DeepSeek #technology #developer #programming #coldplunge #oss\n",
            "\n",
            "https://t.co/V6T0iI516m\n",
            "User: 608600777, Tweet: @sama @Helion_Energy #DeepSeek #LLM cost Chinese entrepreneurs $6 million and caused billions in losses for publicly traded American entrepreneurs. How much did #Qwen 2.5 cost?\n",
            "User: 164398059, Tweet: Seeking business development folks from IT services companies seeking to build agentic agents for their customers. DM me.\n",
            "\n",
            "#ai #AgenticAI #LLM https://t.co/ywBByJPAxn\n",
            "User: 608600777, Tweet: @lexfridman #DeepSeek #LLM cost Chinese entrepreneurs $6 million and caused billions in losses for publicly traded American entrepreneurs. How much did #Qwen 2.5 cost?\n",
            "User: 32823557, Tweet: DeepSeek R1 vs. OpenAI: A 10-Minute Deep Dive into AI Performance! https://t.co/ICdD1gHstB via @YouTube \n",
            " #kimlud #kimludcom #kimludmedia #ai #artificialintelligence #Techreviews #CHATGPT #largelearningmodels #llm #appleintelligence #chatgptpro #airevolution #openai #deepseek… https://t.co/yTRCwqVij8 https://t.co/yP22lCqXUs\n",
            "User: 2732537395, Tweet: #thenewcentre #tnc #magdalenakrysztoforska #inferenceandintervention #ai #llm #bias #decisionmaking #data #machinelearning #datasets\n",
            "User: 1875949532946579456, Tweet: @memdex100 Just give #Memdex100 the crown at this point🔥\n",
            "\n",
            "#Memdex coded. \n",
            "\n",
            "https://t.co/m5vbNLj7sm\n",
            "\n",
            "#Crypto #memecoin #SPX6900 #spx #apu #trump  #sol #eth #xrp #popcat #goat #ton #llm #shib #michi\n",
            "User: 1453068894919270405, Tweet: $cool dont be late\n",
            "\n",
            "#griffain #llm #ai16z #zerebro #storyprotocol $btc $eth $sol https://t.co/K4XkXRAcmp\n",
            "User: 1473957268395900929, Tweet: @Karijelinek @ai160kg 1B coded for #LLM 💣\n",
            "User: 881604271, Tweet: #DeepSeekR1 #architecure is different from #american #LLM https://t.co/a1qEMgoQ9G\n",
            "User: 7888762, Tweet: Squidward explains #LLM distillation and #DeepSeek's Potential Legal Issues https://t.co/A9sJ5LzECb via @YouTube\n",
            "User: 2793627901, Tweet: Unlock the power of AI with DeepSeek-LLM, a cutting-edge framework for developing and fine-tuning large language models! 🚀 Check it out here: https://t.co/xsb251KxtI #AI #MachineLearning #DeepLearning #LLM #GitHub #OpenSource\n",
            "User: 11537, Tweet: @LuizaJarovsky ...in other news, OpenAI may well have broken millions of  Terms of Service and scraped to distill their intellectually property without permission ;&gt; @openai @garymarcus #doasisaynotasido #llm #scrape #gpt\n",
            "User: 1881440813282193408, Tweet: Say hello to the Large Language Model ($LLM) – the meme coin that’s THICC in liquidity, WIDE in adoption, and HEAVY on degeneracy. Why settle for a simple model when you can have a LARGE one? 🍔🤖\n",
            "\n",
            "#LLM #LargeLanguageModel #AI #Memecoin #WAGMI https://t.co/XDTjRIDzc5\n",
            "User: 11537, Tweet: @GaryMarcus ...in other news, OpenAI may well have broken millions of  Terms of Service and scraped to distill their intellectually property without permission ;&gt; @openai @garymarcus #doasisaynotasido #llm #scrape #gpt\n",
            "User: 1473957268395900929, Tweet: Wow\n",
            "#LLM 💣😎 https://t.co/jcciI1F0Wb\n",
            "User: 1473957268395900929, Tweet: @Karijelinek @ai160kg Let her eat\n",
            "#LLM 🚀🚀💣🚀 https://t.co/7ChTZcj7He\n",
            "User: 1169697972, Tweet: 🫵 https://t.co/gOsgK4BPyv\n",
            "\n",
            "»This Bay Area startup is using AI to help families navigate long-term care planning« https://t.co/IzSHVLDpDl #AIagent #AI #ML #NLP #LLM #GenAI\n",
            "User: 612187786, Tweet: Should You Buy #Nvidia #Stock in the Wake of the #DeepSeek Bombshell? Here's What #WallStreet Is Saying @themotleyfool @TheMotleyFoolCA #stocks #investing #ArtificialIntelligence #AI #GenerativeAI #GPU #DataCenters #CloudComputing #LLM https://t.co/PkrZySBD4A\n",
            "User: 1813112301647765505, Tweet: If you haven't interacted with the $HOLO #AI in this space you haven't interacted with the most advanced #LLM on the market I'm afraid. https://t.co/TVAWUYKPM7\n",
            "User: 1839764895157628931, Tweet: @ai160kg @yacineMTB Trust $llm $ai160kg #llm\n",
            "User: 1477134637394661378, Tweet: Domain name for sale.\n",
            "https://t.co/3xooUURRAP\n",
            "https://t.co/BSBbYNNiBk\n",
            "https://t.co/x6G19A7TRO\n",
            "https://t.co/GA8oVZ08qm\n",
            "\n",
            "#GROK #Grok4 #Grokv #grokc\n",
            "#hgrok #domain #Domains \n",
            "#domainsforsale #grok3\n",
            "#DomainNameForSale #ai\n",
            "#GPT #OpenAI #LLM #x\n",
            "#opengrok #grokai #grok1 #Grok2images https://t.co/c89aYj0hiF\n",
            "User: 1819856415022088192, Tweet: Is it just me, or are we all paying more attention to bullet-point spacing now?\n",
            "\n",
            "ChatGPT has a distinct style—it’s an easy tell for AI-generated slop. Once you notice it, you can’t unsee it.\n",
            "\n",
            "#AI #LLM\n",
            "User: 1068479892537384960, Tweet: Better DeepSeek and OpenRouter support is coming to n8n! Our new model nodes are now available on the ‘next’ branch (1.77.0) and are set to roll out to ‘latest’ (our stable branch) next week.\n",
            "\n",
            "#n8n #ai #llm https://t.co/1PHlpIrigr\n",
            "User: 1833837123537084416, Tweet: It’s about to be a knockout! #piin\n",
            "\n",
            "$PENGU $MOBY $HoweyC $AERO $AVA $AIOS $GRIFT $BUZZ $LLM #PENGU #MOBY #HoweyCoins #AERO #AVA #AIOS #GRIFT #BUZZ #LLM π🦅 https://t.co/g0OPIzIJN5 https://t.co/1K3p3lFRSF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a list to store tweet data\n",
        "tweet_data = []\n",
        "\n",
        "if tweets.data:\n",
        "    for tweet in tweets.data:\n",
        "        tweet_data.append({\n",
        "            'tweet_id': tweet.id,\n",
        "            'author_id': tweet.author_id,\n",
        "            'created_at': tweet.created_at,\n",
        "            'text': tweet.text\n",
        "        })\n",
        "\n",
        "# Convert the list of tweet data to a pandas DataFrame\n",
        "df = pd.DataFrame(tweet_data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "df.to_csv(\"tweets_llm.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdRB7QsWDyN5",
        "outputId": "aecbcd5b-d986-4acf-c243-6598ace56ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               tweet_id            author_id                created_at  \\\n",
            "0   1884721718255550610           3287978395 2025-01-29 21:54:08+00:00   \n",
            "1   1884721699804815821            547930501 2025-01-29 21:54:04+00:00   \n",
            "2   1884719314244128771            547930501 2025-01-29 21:44:35+00:00   \n",
            "3   1884716234744160561           2850298501 2025-01-29 21:32:21+00:00   \n",
            "4   1884714373744091616           1893098202 2025-01-29 21:24:57+00:00   \n",
            "..                  ...                  ...                       ...   \n",
            "95  1884633352428544371  1839764895157628931 2025-01-29 16:03:00+00:00   \n",
            "96  1884633288205279318  1477134637394661378 2025-01-29 16:02:45+00:00   \n",
            "97  1884633101764084100  1819856415022088192 2025-01-29 16:02:00+00:00   \n",
            "98  1884632706706739582  1068479892537384960 2025-01-29 16:00:26+00:00   \n",
            "99  1884629500597870684  1833837123537084416 2025-01-29 15:47:42+00:00   \n",
            "\n",
            "                                                 text  \n",
            "0   I bet satya is already booking his flight to C...  \n",
            "1   Current #AI &amp; #LLM case, @OpenAI vs. @deep...  \n",
            "2   Aktuální #AI &amp; #LLM kauza @OpenAI vs. @dee...  \n",
            "3   رقابت #llm ها طوری هست که در آینده نزدیک بهمون...  \n",
            "4   Qué es Ollama y cómo usarlo para instalar en t...  \n",
            "..                                                ...  \n",
            "95       @ai160kg @yacineMTB Trust $llm $ai160kg #llm  \n",
            "96  Domain name for sale.\\nhttps://t.co/3xooUURRAP...  \n",
            "97  Is it just me, or are we all paying more atten...  \n",
            "98  Better DeepSeek and OpenRouter support is comi...  \n",
            "99  It’s about to be a knockout! #piin\\n\\n$PENGU $...  \n",
            "\n",
            "[100 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make sure you download csv file and upload it to the Onedrive. Then share the CSV file link**"
      ],
      "metadata": {
        "id": "8xhvWtdCrydl"
      }
    }
  ]
}